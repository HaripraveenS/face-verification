{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"neural_network.ipynb","provenance":[],"mount_file_id":"1szHWvk72UPfKjyMGIWPDHw7iCtEK6Jn8","authorship_tag":"ABX9TyN52uMIQJmA2VFiaITixJsC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"c1AsAkhqq0WE"},"source":["import torchvision.transforms as transforms\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader, Dataset\n","import torchvision\n","from torchvision import models\n","import torch.optim as optim\n","import pandas as pd\n","import numpy as np\n","import cv2\n","import os\n","from sklearn import preprocessing\n","import matplotlib.pyplot as plt\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LBRVSKj5vN7f"},"source":["def initialize_lfw():\n","    # output_file_path = \"../../data/attribute classifier/lfw/\"\n","    global output_file_path, output_low_level_path, df_attributes\n","    output_file_path = \"/content/drive/MyDrive/project-sepnu/data/neural network/\"\n","    output_low_level_path = '/content/drive/MyDrive/project-sepnu/data/low level/lfw/'\n","    # output_low_level_path = \"../../data/low level/lfw/\"\n","    map_attributes_features_path =\"/content/drive/MyDrive/project-sepnu/data/attribute classifier/lfw/map_attributes_features.txt\"\n","    attributes_path = \"/content/drive/MyDrive/LFW/metadata/modified_attributes.txt\"\n","    if not os.path.exists(map_attributes_features_path):\n","        file_attributes = open(attributes_path, \"r\"); file_map_attributes_features = open(map_attributes_features_path, \"w\")\n","        for attribute in file_attributes.readline().replace(\"\\n\", \"\").split(\",\")[2:]: file_map_attributes_features.write(attribute + \":\" + input(\"Enter space separated features for the attribute \\\"%s\\\": \" % (attribute, )) + \"\\n\")\n","        file_attributes.close(); file_map_attributes_features.close()\n","    file_map_attributes_features = open(map_attributes_features_path, \"r\")\n","    global map_attributes_features \n","    map_attributes_features = {}\n","    for attribute, features in [line.split(\":\") for line in file_map_attributes_features.readlines()]:\n","        map_attributes_features[attribute] = features.split()\n","    file_map_attributes_features.close()\n","    df_attributes = pd.read_csv(attributes_path)\n","    file_images_under_error = open(output_low_level_path + \"images_under_error.txt\", \"r\")\n","    while True:\n","        line = file_images_under_error.readline().replace(\"\\n\", \"\")\n","        if line == \"\": break\n","        line = line.split()[0]\n","        matched_pattern = re.match(r\"^([\\w-]+?)_(\\d+).jpg$\", line[line.rindex(\"/\")+1:])\n","        name, image_num = matched_pattern.groups()\n","        name = name.replace(\"_\", \" \")\n","        image_num = int(image_num)\n","        initial_shape = df_attributes.shape\n","        df_attributes.drop(df_attributes[(df_attributes['person'] == name) & (df_attributes['imagenum'] == image_num)].index, inplace = True)\n","        final_shape = df_attributes.shape\n","        assert (final_shape[0] + 1, final_shape[1]) == initial_shape\n","    file_images_under_error.close()\n","    assert df_attributes.shape[0] == np.load(output_low_level_path + \"rgb_chin.npy\").shape[0]\n","    \n","\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZlLz6DUjrTXb"},"source":["class lfwData(Dataset):\n","    def __init__(self,data_list,attribute,transform=None,train=True):\n","        super().__init__()\n","        global map_attributes_features\n","        self.data_list = data_list\n","        # self.data_dir = data_dir\n","        self.transform = transform\n","        self.train = train\n","        # self.df_attr = df_attributes\n","        self.attribute = attribute\n","        self.features = map_attributes_features[attribute]\n","        self.map_attr = df_attributes.columns.tolist().index(attribute)\n","\n","    \n","    def __len__(self):\n","        return len(self.data_list)\n","    \n","    def __getitem__(self,item):\n","        global output_file_path, output_low_level_path, df_attributes\n","        # print(item)\n","        img_idx = item\n","        p1feature = []\n","        for feature in self.features:\n","            feature_array = np.load(output_low_level_path + feature + \".npy\")\n","            p1feature.extend(feature_array[img_idx]/feature_array[img_idx].sum())\n","            # p1feature = np.hstack((p1feature,feature_array[img_idx]))\n","        \n","        label = df_attributes.iloc[img_idx,self.map_attr]\n","        label = max(label,0)\n","        p1feature = np.array(p1feature)\n","        if self.transform is not None:\n","            p1feature = self.transform(p1feature)\n","        if self.train:\n","          return {\n","              'feat' : p1feature,\n","              'label' : torch.tensor(label)\n","          }\n","        else:\n","          return {\n","              'feat':p1feature\n","          }\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h4Qrw2XICBJr"},"source":["def get_datalist(df):\n","    # df = df_attributes\n","    lfwnames = df['person'].tolist()\n","    lfwnames_nums = df['imagenum'].tolist()\n","    lfwnames_act = []\n","    for i in range(len(lfwnames)):\n","        num = '{0:04}'.format(lfwnames_nums[i])\n","        lfwnames_act.append(lfwnames[i].replace(\" \",\"_\") + \"_\" + num + \".jpg\")\n","    assert len(lfwnames_act) == df.shape[0]\n","    return lfwnames_act\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xvVPFyFRAq9A"},"source":["initialize_lfw()\n","# print(df_attributes.head())\n","data_list = get_datalist(df_attributes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ciaMclI3Cq5T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619025244012,"user_tz":-330,"elapsed":3407,"user":{"displayName":"sepnu paus","photoUrl":"","userId":"11897582063931328693"}},"outputId":"514123dc-920e-474b-e756-fd214b20e65f"},"source":["transforms_train = transforms.Compose([\n","    transforms.ToTensor(),\n","    # transforms.Normalize((0.3166, 0.3947, 0.4725), (0.1755, 0.1720, 0.1657)),\n","])\n","# transform = transforms.ToTensor()\n","train_data = lfwData(data_list,\"Asian\")\n","# len(train_data)\n","tem = train_data.__getitem__(3)\n","tem[\"feat\"].shape\n","# print(map_attributes_features.keys())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(400,)"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"G2f3MsuXDKmm"},"source":["batch = 128\n","test_size = 0.3\n","num = train_data.__len__()\n","indices = list(range(num))\n","np.random.shuffle(indices)\n","split = int(np.floor(test_size*num))\n","train_idx,test_idx = indices[split:], indices[:split]\n","\n","#Create Samplers\n","train_sampler = SubsetRandomSampler(train_idx)\n","test_sampler = SubsetRandomSampler(test_idx)\n","train_loader = DataLoader(train_data, batch_size = batch, sampler = train_sampler)\n","test_loader = DataLoader(train_data, batch_size = batch, sampler = test_sampler)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5wwVPVYVsLe1"},"source":["class BasicNet(nn.Module):\n","\tdef __init__(self,in_feats):\n","\t\tsuper().__init__()\n","\t\tself.features = nn.Sequential(\n","\t\t\tnn.Linear(in_feats, 2048),\n","\t\t\tnn.ReLU(),\n","\t\t\tnn.Linear(2048, 4096),\n","\t\t\tnn.Dropout(p=0.5),\n","\t\t\tnn.ReLU(),\n","            # nn.BatchNorm1d(2048),\n","            nn.Linear(4096, 8192),\n","\t\t\tnn.ReLU(),\n","            nn.Linear(8192, 4096),\n","\t\t\tnn.ReLU(),\n","            nn.Linear(4096, 2048),\n","\t\t\tnn.ReLU(),\n","\t\t\tnn.Linear(2048, 1024),\n","            nn.ReLU(),\n","\t\t\tnn.Linear(1024, 512),\n","            nn.ReLU(),\n","\t\t\tnn.Linear(512, 64),\n","            nn.ReLU(),\n","\t\t\tnn.Linear(64, 2),\n","\t\t)\n","\n","\tdef forward(self, x):\n","\t\tx = self.features(x)\n","\t\treturn x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xcU6in6zuxUV","executionInfo":{"status":"ok","timestamp":1619024876655,"user_tz":-330,"elapsed":1432,"user":{"displayName":"sepnu paus","photoUrl":"","userId":"11897582063931328693"}},"outputId":"8169dbd1-ef41-4125-bf39-51b3a2331ac1"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Assuming that we are on a CUDA machine, this should print a CUDA device:\n","\n","\n","print(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BuxPzE9EIqkY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619025299498,"user_tz":-330,"elapsed":2610,"user":{"displayName":"sepnu paus","photoUrl":"","userId":"11897582063931328693"}},"outputId":"5dab0d3d-02e6-4999-ed49-5a7d23b3699e"},"source":["model = BasicNet(400).to(device)\n","error = nn.CrossEntropyLoss().to(device)\n","optimizer = optim.Adam(model.parameters())\n","print(model)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["BasicNet(\n","  (features): Sequential(\n","    (0): Linear(in_features=400, out_features=2048, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=2048, out_features=4096, bias=True)\n","    (3): Dropout(p=0.5, inplace=False)\n","    (4): ReLU()\n","    (5): Linear(in_features=4096, out_features=8192, bias=True)\n","    (6): ReLU()\n","    (7): Linear(in_features=8192, out_features=4096, bias=True)\n","    (8): ReLU()\n","    (9): Linear(in_features=4096, out_features=2048, bias=True)\n","    (10): ReLU()\n","    (11): Linear(in_features=2048, out_features=1024, bias=True)\n","    (12): ReLU()\n","    (13): Linear(in_features=1024, out_features=512, bias=True)\n","    (14): ReLU()\n","    (15): Linear(in_features=512, out_features=64, bias=True)\n","    (16): ReLU()\n","    (17): Linear(in_features=64, out_features=2, bias=True)\n","  )\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JLJ5mRwPJbVE","executionInfo":{"status":"ok","timestamp":1619025300218,"user_tz":-330,"elapsed":1652,"user":{"displayName":"sepnu paus","photoUrl":"","userId":"11897582063931328693"}},"outputId":"74ffca68-be88-43e2-e1d9-fb02b63802db"},"source":["rand_tensor = 8*torch.rand((64,400))\n","temp = rand_tensor.to(device)\n","#print tensor\n","# print(rand_tensor)\n","# temp = temp.view(200,64)\n","print(temp.shape)\n","y = model(temp)\n","# print(rand_tensor)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([64, 400])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HJhnODeUIrcH","executionInfo":{"status":"ok","timestamp":1619028633992,"user_tz":-330,"elapsed":3329404,"user":{"displayName":"sepnu paus","photoUrl":"","userId":"11897582063931328693"}},"outputId":"7004f20f-269b-4476-f5ef-74e02ddacae3"},"source":["n_epochs = 5\n","valid_loss_min = np.Inf\n","\n","train_losses = []\n","valid_losses = []\n","\n","for epoch in range(n_epochs):\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","    \n","    model.train()\n","    for feats in train_loader:\n","        data = feats['feat'].squeeze(0).to(device)\n","        target = feats['label'].to(device)\n","        optimizer.zero_grad()\n","        output = model(data.float())\n","        loss = error(output,target)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()*data.size(0)\n","    \n","     \n","    train_loss /= len(train_loader.sampler)\n","    # valid_loss /= len(valid_loader.sampler)\n","    \n","    train_losses.append(train_loss)\n","    # valid_losses.append(valid_loss)\n","    \n","    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n","        epoch, train_loss))\n","    \n","    # if valid_loss <= valid_loss_min:\n","        # print(\"Validation Loss decreased {:0.6f} -> {:0.6f}\".format(valid_loss_min,valid_loss))\n","        # valid_loss_min = valid_loss/\n","        # torch.save(model.state_dict, 'best_model_so_far.pth')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 0 \tTraining Loss: 0.294370\n","Epoch: 1 \tTraining Loss: 0.272487\n","Epoch: 2 \tTraining Loss: 0.276515\n","Epoch: 3 \tTraining Loss: 0.260960\n","Epoch: 4 \tTraining Loss: 0.247850\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uS4Qc8nWOHTp"},"source":["# df_attributes.loc[6043,\"Male\"]\n","torch.save(model.state_dict, '/content/drive/MyDrive/project-sepnu/data/neural network/asian_lfw.pth')\n","# df_attributes.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ydrlhM5M0oi7","executionInfo":{"status":"ok","timestamp":1619028944207,"user_tz":-330,"elapsed":290436,"user":{"displayName":"sepnu paus","photoUrl":"","userId":"11897582063931328693"}},"outputId":"b05a3020-1fc3-43b6-f046-19b24d9646c9"},"source":["# model.load_state_dict(torch.load('/content/drive/MyDrive/project-sepnu/data/neural network/Male_lfw.pth'))\n","model.eval()\n","correct = 0\n","total = 0\n","pred_list = []\n","correct_list = []\n","with torch.no_grad():\n","    for images in test_loader:\n","        data = images['feat'].squeeze(0).to(device)\n","        target = images['label'].to(device)\n","        outputs = model(data.float())\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += target.size(0)\n","        pr = predicted.detach().cpu().numpy()\n","        for i in pr:\n","          pred_list.append(i)\n","        tg = target.detach().cpu().numpy()\n","        for i in tg:\n","          correct_list.append(i)\n","        correct += (predicted == target).sum().item()\n","\n","\n","print('Accuracy of the network on the 10000 test images: %f %%' % (\n","    100 * correct / total))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy of the network on the 10000 test images: 91.854508 %\n"],"name":"stdout"}]}]}